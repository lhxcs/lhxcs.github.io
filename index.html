<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Hanxuan Li 李瀚轩</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Hanxuan Li</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="pub.html">Publications</a></div>
<div class="menu-item"><a href="miscellaneous.html">Miscellaneous</a></div>
<!-- <div class="menu-item"><a href="fun.html">Fun</a></div> -->
<div class="menu-item"><a href="CV_YipingWang_phd.pdf">CV</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Hanxuan Li 李瀚轩</h1>
</div>
<table class="imgtable"><tr><td>
<!-- <img src="photos/sunshine2.png" alt="alt text" width="190px" height="240px" />&nbsp;</td> -->
<img src="images/photo.jpg" alt="alt text" width="240px" height="240px" />&nbsp;</td>
<td align="left"><p>Hanxuan Li<br />
Undergraduate student<br /> <a href="http://www.cs.zju.edu.cn/turingclass_en/">Turing Class</a>, <a href="http://www.en.cs.zju.edu.cn/">College of Computer Science and Technology</a>, <br />
<a href="https://www.zju.edu.cn/english/">Zhejiang University</a><br />
Email: lihanxuan23@gmail.com <br /><br />
<a href="https://github.com/lhxcs">Github</a> / <a href="https://lhxcs.github.io/note/">Notebook</a><br /></p>
</td></tr></table>
<h2>About me</h2>
<p> I'm a third year undergraduate student (2022.9 - Present) in <a href="http://www.cs.zju.edu.cn/turingclass_en/">Turing Class</a>, <a href="http://ckc.zju.edu.cn/ckcen/">Chu Kochen Honors College</a>, <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>. </p>
<p> My research interests now focus on two directions in deep learning: </p>
<ul>
        <li>Investigating the mathematical principles behind deep learning models to explain its empirical successes and failure modes.</li>
        <li>Designing efficient algorithms to accelerate inference/training and reduce memory overhead.</li>
    </ul>
<p> I have studied <b>post-training quantization</b> on video generation models (<a href="http://export.arxiv.org/abs/2505.18663">DVD-Quant</a>). I'm also exploring RL for LLM reasoning recently.</p>
<h2>News</h2>
<ul>
</ul>

<!-- <h2>My Favourite Papers</h2> -->
<h2>Research directions and Selected Papers</h2>
<p><span class="preserve-space">(* denotes equal contribution, &dagger; denotes corresponding author)</span> <br /><br /></p>
<p><div class="boxed">
</p>

<table class="imgtable"><tr><td>
<img src="images/paper/dvd_quant.png" alt="alt text" width="300px" height="200px" />&nbsp;</td>
<td align="left"><p><a href="https://arxiv.org/abs/2505.18663">
  <b>DVD-Quant: Data-free Video Diffusion Transformers Quantization</b>
</a> 
<br>
Zhiteng Li*, <b>Hanxuan Li</b>*, Junyi Wu, Kai Liu, Linghe Kong, Guihai Chen, Yulun Zhang, Xiaokang Yang
<br>
<i>Preprint</i>
<br>
<a href="https://arxiv.org/abs/2505.18663" style="color: #6bbed4ee">[Arxiv]</a> 
  <a href="https://github.com/lhxcs/DVD-Quant" style="color: #6bbed4ee">[Code]</a> 
<br><br>
tl;dr: We develop the first data-free quantization framework that achieves W4A4 PTQ for Video DiTs with 2× speedup while maintaining visual fidelity.
<!-- tl;dr: We design universal data selection methods for CLIP pretraining and achieve near SOTA results with less than 10% of preprocessing resources. It can obtain a new SOTA in <a href="https://www.datacomp.ai/dcclip/leaderboard.html">DataComp benchmark</a> when combined with other approaches.</p> -->
</td></tr></table>
<p></div></p>


</td>
</tr>
</table>

</body>
</html>